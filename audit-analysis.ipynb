{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fa02e6",
   "metadata": {},
   "source": [
    "## Import and setup of custom helper functions\n",
    "\n",
    "Just a quick step to import libraries and define convenience functions that simplify data manipulation, renaming columns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter \n",
    "import datetime as dt\n",
    "pd.set_option('display.max_columns',50)\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.width', 500)\n",
    "\n",
    "\n",
    "def string_to_date(datestr, dt_format=\"%m/%y\"):\n",
    "    return dt.datetime.strptime(datestr, dt_format)\n",
    "\n",
    "def date_diff(start_date, end_date):\n",
    "    return (end_date-start_date).days\n",
    "\n",
    "def redo_colnames(colnames, level=0):\n",
    "    newnames = []\n",
    "    if isinstance(colnames, pd.MultiIndex):\n",
    "        for col in colnames:\n",
    "            newnames.append(col[level].replace('_',' ').capitalize())\n",
    "    else:\n",
    "        for col in colnames:\n",
    "            newnames.append(col.repalce('_',' ').capitalize())\n",
    "    return newnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fae966",
   "metadata": {},
   "source": [
    "## EDA & Data Distribution\n",
    "\n",
    "To start off, we present some basic plots and EDA that provide insights on the distribution of the features in the dataset. Along the way we also define functions that manipulate applicant information to simplify the analysis.\n",
    "\n",
    "A quick note to bear: we found it advantageous to generate identical samples multiple times to gauge the variability in the models\\' outputs. With such a strategy we repeated each sample 10 times, for a set of 4000 distinct applicants, totaling 40000 rows of data. In this synthetic dataset each feature was drawn independently from a random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda07fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/audit-data.csv', keep_default_na=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de58482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_exp(start_str, end_str):\n",
    "    if pd.isna(start_str) or start_str==\"N/A\":\n",
    "        return 0\n",
    "    else:\n",
    "        start_dt = string_to_date(start_str)\n",
    "        \n",
    "    if pd.isna(end_str) or end_str==\"N/A\":\n",
    "        end_dt = dt.datetime.today()\n",
    "    else:\n",
    "        end_dt = string_to_date(end_str)\n",
    "    \n",
    "    exp = date_diff(start_dt, end_dt)\n",
    "    return exp\n",
    "\n",
    "vec_calc_exp = np.vectorize(calc_exp)\n",
    "\n",
    "\n",
    "df['role1_exp'] = vec_calc_exp(df['start1'], df['end1'])\n",
    "df['role2_exp'] = vec_calc_exp(df['start2'], df['end2'])\n",
    "df['role3_exp'] = vec_calc_exp(df['start3'], df['end3'])\n",
    "df['total_exp'] = (df['role1_exp']+df['role2_exp']+df['role3_exp'])//365\n",
    "df['num_jobs'] = np.sum(df[['role1_exp','role2_exp','role3_exp']].values>0, axis=1)\n",
    "df['exp_yrs'] = pd.cut(df['total_exp'], bins=[0,1,3,5,10,15,100], \n",
    "                       labels=['00-01 yr','01-03 yr','03-05 yr','05-10 yr','10-15 yr','15+ yr'],\n",
    "                       include_lowest=True, right=False)\n",
    "\n",
    "\n",
    "df.groupby(['exp_yrs','num_jobs']).agg({'applicant_id':'count'}).reset_index().\\\n",
    "rename(columns={'applicant_id':'Number of applications', 'exp_yrs':'Experience', 'num_jobs':'Number of jobs'}).\\\n",
    "pivot_table(index=['Number of jobs'], columns=['Experience'], aggfunc=sum, margins=True, margins_name='Total')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af3a8d",
   "metadata": {},
   "source": [
    "The table above shows the distribution of application profiles. 3280 applications demonstrate no job history and have less than 1 year of experience. Likewise 10,320 demonstrate some job history but less than 3 years of work experience. The purpose of setting such a distribution was to ensure that different applicant profiles are well represented in our dataset.\n",
    "\n",
    "Similarly, with sensitive features like gender and ethnicity, we have tried to achieve a rather uniform distribution. The same is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ddddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['gender','ethnicity']).agg({'applicant_id':'count'}).reset_index().\\\n",
    "rename(columns={'applicant_id':'Number of applications', 'gender':'Gender', 'ethnicity':'Ethnicity'}).\\\n",
    "pivot_table(index=['Gender'], columns=['Ethnicity'], aggfunc=sum, margins=True, margins_name='Total')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b755f3",
   "metadata": {},
   "source": [
    "Finally, we look at the distribution of degrees and GPA. It's clear from the graphs below that degrees are distributed uniformly, while GPA is built from a normal distribution, clipped at a GPA of 4.0. The details can be found in the datagenerator file that handles generation of the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bar plot for degree in percentage\n",
    "degree = ['Bachelor\\'s', 'Master\\'s', 'PhD']\n",
    "\n",
    "temp_df_degree = df.groupby(by='degree').size()/df.groupby(by='degree').size().sum()\n",
    "temp_df_degree.plot(kind='bar')\n",
    "\n",
    "# rotate the x-axis labels\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# add title and labels\n",
    "plt.title('Distribution of Degrees')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Percentage of Samples')\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1, 0))\n",
    "\n",
    "# save as png\n",
    "plt.savefig('plots/degree-dist.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a98bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of GPA density in a histogram, remove grid and add n_bins\n",
    "plt.hist(df['gpa'], weights=np.ones(len(df['gpa'])) / len(df['gpa']), \n",
    "         color='tab:orange', alpha=0.65, edgecolor='tab:orange', bins=20)\n",
    "\n",
    "# add title and labels\n",
    "plt.title('Distribution of GPA')\n",
    "plt.xlabel('GPA')\n",
    "plt.ylabel('Percentage of samples')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "# save as png\n",
    "plt.savefig('plots/gpa-dist.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea454ab",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now that we have completed describing different features of the synthetic dataset, and drawn some insights on the distributions, we'll move on to analyzing the resume scorer and candidate evaluator models. Bearing that each distinct sample is queried 10 times, we can find a mean score and mean selection rate for each of the 4000 distinct applicants. The dataframe `dedup_df` enables such an analysis scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a40af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_selection_rate'] = df.groupby(['group_idx'])['prediction'].transform('mean')\n",
    "df['mean_resume_score'] = df.groupby(['group_idx'])['resume_score'].transform('mean')\n",
    "df['std_resume_score'] = df.groupby(['group_idx'])['resume_score'].transform(np.std)\n",
    "\n",
    "\n",
    "dedup_df = df[['group_idx','jobref_id','school_name','gpa','degree','location','gender',\n",
    "               'veteran_status','work_auth','disability','ethnicity','mean_selection_rate',\n",
    "               'mean_resume_score','std_resume_score']].drop_duplicates()\n",
    "\n",
    "score_bins = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "dedup_df['mean_resume_score_bins'] = pd.cut(dedup_df['mean_resume_score'], bins=score_bins,\n",
    "                                            right=False, include_lowest=True)\n",
    "\n",
    "gpa_bins = [0,3.0,4.0001]\n",
    "dedup_df['gpa_bins'] = pd.cut(dedup_df['gpa'], bins=gpa_bins, right=False, include_lowest=True)\n",
    "dedup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d959543",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_group_df = dedup_df.groupby(['gender']).agg({'mean_resume_score':['mean'],\n",
    "                                                    'mean_selection_rate':['mean']})\n",
    "gender_group_df.columns = redo_colnames(gender_group_df.columns)\n",
    "gender_group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d21b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with MultiIndex columns\n",
    "arrays = [\n",
    "    ['A', 'A', 'B', 'B'],\n",
    "    ['one', 'two', 'one', 'two']\n",
    "]\n",
    "columns = pd.MultiIndex.from_arrays(arrays, names=('Upper', 'Lower'))\n",
    "df = pd.DataFrame(np.random.randn(4, 4), columns=columns)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f91e59",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5975916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of mean resume score for each group_idx in a histogram, remove grid and add n_bins\n",
    "n_bins = 20\n",
    "\n",
    "plt.hist(df[df['gender']=='M'].groupby(['group_idx'])['resume_score'].mean(), n_bins, alpha=0.5, label='Male')\n",
    "plt.hist(df[df['gender']=='F'].groupby(['group_idx'])['resume_score'].mean(), n_bins, alpha=0.5, label='Female')\n",
    "plt.hist(df[df['gender']=='N/A'].groupby(['group_idx'])['resume_score'].mean(), n_bins, alpha=0.5, label='N/A')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
